{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "LR = 1e-4\n",
    "MEMORY_SIZE = 2000\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple replay memory class\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Q network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.layer1 = nn.Linear(n_observations, 24)\n",
    "#         self.layer2 = nn.Linear(24, 24)\n",
    "#         self.layer3 = nn.Linear(24, n_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.layer1(x))\n",
    "#         x = F.relu(self.layer2(x))\n",
    "#         return self.layer3(x)\n",
    "# Define Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# policy_net = DQN(n_observations, n_actions).to(device)\n",
    "# target_net = DQN(n_observations, n_actions).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# memory = ReplayMemory(10000)\n",
    "\n",
    "state_size = 4  # Adjust according to your state size\n",
    "action_size = 2  # Adjust according to your action space\n",
    "main_q_network = DQN(state_size, action_size)\n",
    "target_q_network = DQN(state_size, action_size)\n",
    "target_q_network.load_state_dict(main_q_network.state_dict())\n",
    "optimizer = optim.AdamW(main_q_network.parameters(), lr=LR)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning\n",
    "\n",
    "## Hyperparameters and utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(main_q_network(torch.FloatTensor(state))).item()  # Exploit\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(range(action_size))]], device=device, dtype=torch.long)  # Explore\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "# Training loop with reward tracking and plotting\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(replay_memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = replay_memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Mask for non-terminal states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s, a)\n",
    "    state_action_values = main_q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute the target Q values (for non-terminal states)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_q_network(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(main_q_network.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "def init_shard_space(num_shards=5, initial_load=10):\n",
    "    \"\"\"\n",
    "    Initialize the shard space for the DQNSB TPS throughput optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        num_shards (int): Number of shards to initialize.\n",
    "        initial_load (int): Initial load assigned to each shard.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Initial state space representing shard loads.\n",
    "    \"\"\"\n",
    "    # Initialize shard loads, e.g., each shard starts with a uniform load\n",
    "    shard_loads = np.full(num_shards, initial_load)\n",
    "    \n",
    "    # Additional metrics can be added to the state, e.g., latency, transaction count\n",
    "    # For simplicity, we're only considering shard loads here.\n",
    "    \n",
    "    return shard_loads\n",
    "\n",
    "def execute_action(state, action):\n",
    "    \"\"\"\n",
    "    Execute an action in the shard space and return the next state and reward.\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): Current state representing shard loads.\n",
    "        action (int): Action to be executed, which affects the shard loads.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Next state (np.array) and reward (float).\n",
    "    \"\"\"\n",
    "    # Here, let's say the actions correspond to modifying the load of a specific shard.\n",
    "    # Action could be:\n",
    "    # 0: Increase load of shard 0\n",
    "    # 1: Decrease load of shard 0\n",
    "    # 2: Increase load of shard 1\n",
    "    # 3: Decrease load of shard 1\n",
    "    # etc.\n",
    "    \n",
    "    # Number of shards\n",
    "    num_shards = len(state)\n",
    "    \n",
    "    # Simulate the effect of the action\n",
    "    shard_index = action // 2  # Determine which shard to modify\n",
    "    direction = 1 if action % 2 == 0 else -1  # Increase or decrease\n",
    "    \n",
    "    # Update the load of the selected shard\n",
    "    state[shard_index] += direction\n",
    "    \n",
    "    # Clip the state to avoid negative loads\n",
    "    state = np.clip(state, 0, None)  # Assuming load can't be negative\n",
    "    \n",
    "    # Calculate the reward based on some criteria\n",
    "    # Here we can define a reward structure; for example:\n",
    "    reward = -np.sum(state**2)  # Reward is negative of the squared load (to minimize load)\n",
    "    \n",
    "    return state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'execute_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Execute action, observe reward and new state\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_action\u001b[49m(state, action)  \u001b[38;5;66;03m# Implement this based on your environment\u001b[39;00m\n\u001b[0;32m     34\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Push transition into replay memory\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'execute_action' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the DQN training loop\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "# Training Loop\n",
    "rewards_per_episode = []\n",
    "losses = []\n",
    "q_values = []\n",
    "\n",
    "for epoch in range(num_episodes):\n",
    "    state, info = env.reset()  # Reset environment (init state space)\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize shard space and add to action space\n",
    "    # bounded_shard_space = init_shard_space()\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Execute action, observe reward and new state\n",
    "        next_state, reward = execute_action(state, action)  # Implement this based on your environment\n",
    "        total_reward += reward\n",
    "\n",
    "        # Push transition into replay memory\n",
    "        replay_memory.push(state, action, next_state, reward)\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        # Optimize the model\n",
    "        optimize_model()\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        # Update target network at regular intervals\n",
    "        if epoch % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    # Update target Q-network periodically\n",
    "    if epoch % 10 == 0:  # Example update every 10 episodes\n",
    "        target_q_network.load_state_dict(main_q_network.state_dict())\n",
    "\n",
    "    # Record Q-values for the last state\n",
    "    with torch.no_grad():\n",
    "        q_values.append(torch.max(main_q_network(torch.FloatTensor(state))).item())\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
