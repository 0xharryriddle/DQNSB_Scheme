{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Imports and settings\n",
    "# \n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import threading\n",
    "from Cryptodome.PublicKey import RSA\n",
    "from Cryptodome.Signature import PKCS1_v1_5\n",
    "from Cryptodome.Hash import SHA256\n",
    "from secrets import SystemRandom\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n : number of processors\n",
    "n = 150\n",
    "# s - where 2^s is the number of committees\n",
    "s = 4\n",
    "# c - size of committee\n",
    "c = 2\n",
    "# D - difficulty level , leading bits of PoW must have D 0's (keep w.r.t to hex)\n",
    "D = 1 \n",
    "# r - number of bits in random string \n",
    "r = 5\n",
    "# fin_num - final committee id\n",
    "fin_num = 0\n",
    "# identityNodeMap- mapping of identity object to Elastico node\n",
    "identityNodeMap = dict()\n",
    "# commitmentSet - set of commitments S\n",
    "commitmentSet = set()\n",
    "# ledger - ledger is the database that contains the set of blocks where each block comes after an epoch\n",
    "ledger = []\n",
    "# NtwParticipatingNodes - list of nodes those are the part of some committee\n",
    "NtwParticipatingNodes = []\n",
    "# network_nodes - list of all nodes \n",
    "network_nodes = []\n",
    "# ELASTICO_STATES - states reperesenting the running state of the node\n",
    "ELASTICO_STATES = {\"NONE\": 0, \"PoW Computed\": 1, \"Formed Identity\" : 2,\"Formed Committee\": 3, \"RunAsDirectory\": 4 ,\"Receiving Committee Members\" : 5,\"Committee full\" : 6 , \"PBFT Finished\" : 7, \"Intra Consensus Result Sent to Final\" : 8, \"Final Committee in PBFT\" : 9, \"FinalBlockSent\" : 10, \"FinalBlockReceived\" : 11, \"RunAsDirectory after-TxnReceived\" : 12, \"RunAsDirectory after-TxnMulticast\" : 13, \"Final PBFT Start\" : 14, \"Merged Consensus Data\" : 15, \"PBFT Finished-FinalCommittee\" : 16 , \"CommitmentSentToFinal\" : 17, \"BroadcastedR\" : 18, \"ReceivedR\" :  19, \"FinalBlockSentToClient\" : 20}\n",
    "\n",
    "# Initialize DQN parameters\n",
    "state_size = env.observation_space.shape[0] # size of state space\n",
    "action_size = env.action_space.n # size of action space\n",
    "replay_memory = deque(maxlen=1000)\n",
    "epsilon = 0.1  # exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, transmission_rate, computing_capabilities, consensus_history):\n",
    "        self.transmission_rate = transmission_rate  # R(i,j) between nodes i and j\n",
    "        self.computing_capabilities = computing_capabilities  # Computing power of nodes\n",
    "        self.consensus_history = consensus_history  # H: Consensus validity history\n",
    "        \n",
    "    def compute_malicious_node_probability(self): \n",
    "        \"\"\"\n",
    "        Compute p̄ (malicious node probability) based on consensus history (H)\n",
    "        \"\"\"\n",
    "        trust = self.compute_network_trust()\n",
    "        return 1 - trust\n",
    "        \n",
    "    def compute_network_trust(self):\n",
    "        \"\"\"\n",
    "        Normalize consensus history H and calculate trust level (binary encoding 1 = valid, 0 = invalid)\n",
    "        \"\"\"\n",
    "        total_nodes = len(self.consensus_history)\n",
    "        valid_consensus_count = sum(self.consensus_history)\n",
    "        return valid_consensus_count / total_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(object):\n",
    "    def __init__(self, max_size, max_interval, max_shards):\n",
    "        self.max_size = max_size\n",
    "        self.max_interval = max_interval\n",
    "        self.max_shards = max_shards\n",
    "\n",
    "    def select_actions(self):\n",
    "        \"\"\"\n",
    "        DQN selects block size, block interval, and number of shards\n",
    "        \"\"\"\n",
    "        block_size = random.randint(1, self.max_size)\n",
    "        block_interval = random.randint(1, self.max_interval)\n",
    "        num_shards = random.randint(1, self.max_shards)\n",
    "        return block_size, block_interval, num_shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockchain environment (BCenv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Hyper parameters and utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128  # Reduced batch size for faster updates and less memory usage\n",
    "GAMMA = 0.95  # Discount factor for future rewards, slightly reduced to prioritize immediate rewards\n",
    "EPS_START = 1.0  # Start with full exploration\n",
    "EPS_END = 0.05  # End with a small amount of exploration\n",
    "EPS_DECAY = 5000  # Faster decay to reduce exploration over time\n",
    "TAU = 0.01  # Increase soft update rate for target network\n",
    "LR = 5e-4  # Learning rate, increased for faster learning\n",
    "\n",
    "# Replay memory capacity\n",
    "memory = ReplayMemory(5000)  # Reduced capacity to save memory\n",
    "\n",
    "# Initialize networks\n",
    "main_q_network = DQN(state_size, action_size).to(device)\n",
    "target_q_network = DQN(state_size, action_size).to(device)\n",
    "target_q_network.load_state_dict(main_q_network.state_dict())\n",
    "target_q_network.eval()  # Set target network to evaluation mode\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(main_q_network.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q∗(S, A) = \\max_π E[\\sum_{t = 0}^{\\infty} γ^tR(S^t, A^t) | S^0 = S, A^0 = A; π]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "tensor([[1]]) (<class 'torch.Tensor'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_reward \u001b[38;5;241m/\u001b[39m steps  \u001b[38;5;66;03m# Example calculation\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mtps_throughput_optimization_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m     90\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[1;32mIn[24], line 51\u001b[0m, in \u001b[0;36mtps_throughput_optimization_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m     50\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(state)\n\u001b[1;32m---> 51\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32md:\\NT547\\ELASTICO-Blockchain\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\NT547\\ELASTICO-Blockchain\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NT547\\ELASTICO-Blockchain\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32md:\\NT547\\ELASTICO-Blockchain\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    210\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    211\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[1;32md:\\NT547\\ELASTICO-Blockchain\\.venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m    134\u001b[0m         action\n\u001b[0;32m    135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "\u001b[1;31mAssertionError\u001b[0m: tensor([[1]]) (<class 'torch.Tensor'>) invalid"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return main_q_network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(action_size)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model(memory, BATCH_SIZE, GAMMA, main_q_network, target_q_network, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = main_q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_q_network(non_final_next_states).max(1)[0]\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(main_q_network.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "# Initialize a list to store throughput data\n",
    "throughput_data = []\n",
    "\n",
    "def tps_throughput_optimization_model():\n",
    "    num_episodes = 1000\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        for t in count():\n",
    "            action = select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward.item()\n",
    "\n",
    "            optimize_model()\n",
    "\n",
    "            target_net_state_dict = target_q_network.state_dict()\n",
    "            policy_net_state_dict = main_q_network.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_q_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate throughput (TPS) for the episode\n",
    "        throughput = calculate_throughput(total_reward, t)\n",
    "        throughput_data.append(throughput)\n",
    "\n",
    "    print('Complete')\n",
    "    env.render()\n",
    "    env.close()\n",
    "\n",
    "def calculate_throughput(total_reward, steps):\n",
    "    if steps == 0:\n",
    "        return 0\n",
    "    return total_reward / steps  # Example calculation\n",
    "\n",
    "# Run the model\n",
    "tps_throughput_optimization_model()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(throughput_data, label='DQN-based Scheme')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Throughput (TPS)')\n",
    "plt.title('TPS Performance and Convergence Trend Analysis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
