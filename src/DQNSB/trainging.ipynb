{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from keras.src import Sequential, optimizers\n",
    "from keras.src.layers import Dense\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockchainEnvironment(Env):\n",
    "    def __init__(self, num_shards, block_size, block_interval, malicious_ratio):\n",
    "        self.num_shards = num_shards      # Number of shards in the system\n",
    "        self.block_size = block_size      # Size of each block (in MB)\n",
    "        self.block_interval = block_interval  # Block interval (in seconds)\n",
    "        self.malicious_ratio = malicious_ratio  # Ratio of malicious nodes\n",
    "        self.state = self.reset()  # Initial state\n",
    "        self.action_space = [(1, 4), (4, 8), (8, 16)]  # Actions: (min_block_interval, max_block_size)\n",
    "        self.max_shards = 10      # Maximum number of shards allowed\n",
    "        self.max_block_size = 8    # Maximum block size (MB)\n",
    "        self.max_block_interval = 16  # Maximum block interval (seconds)\n",
    "        self.tps_baseline = 10  # TPS baseline value\n",
    "        self.total_transactions = 100000  # Total number of transactions\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to an initial state\"\"\"\n",
    "        self.num_shards = 1\n",
    "        self.block_size = 1\n",
    "        self.block_interval = 1\n",
    "        self.malicious_ratio = random.uniform(0.0, 0.3)  # Randomly set a malicious ratio (up to 30%)\n",
    "        return (self.num_shards, self.block_size, self.block_interval, self.malicious_ratio)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action and return the next state, reward, and done flag.\n",
    "        Actions are represented as adjustments to block size, block interval, and shards.\n",
    "        \"\"\"\n",
    "        # get_action = self.action_space[action]\n",
    "        # shard_change = get_action[:0] \n",
    "        # block_interval_change = get_action[:1]\n",
    "        shard_change, block_interval_change = self.action_space[action]\n",
    "        \n",
    "        # Adjust number of shards and block interval based on the action\n",
    "        self.num_shards = min(self.max_shards, max(1, self.num_shards + shard_change))\n",
    "        self.block_size = min(self.max_block_size, max(1, self.block_size + block_interval_change))\n",
    "        self.block_interval = min(self.max_block_interval, max(1, self.block_interval))\n",
    "\n",
    "        # Simulate the environment dynamics based on the new configuration\n",
    "        tps = self.calculate_tps(self.num_shards, self.block_size, self.block_interval)\n",
    "        latency = self.calculate_latency(self.block_size, self.num_shards)\n",
    "        security = self.calculate_security(self.num_shards, self.malicious_ratio)\n",
    "\n",
    "        # Compute the reward (consider TPS, security, and latency)\n",
    "        reward = self.compute_reward(tps, latency, security)\n",
    "\n",
    "        # Check if done (after enough transactions processed)\n",
    "        done = self.total_transactions <= 0\n",
    "\n",
    "        # Update total transactions\n",
    "        self.total_transactions -= tps * self.block_interval\n",
    "\n",
    "        # Next state is the updated blockchain configuration\n",
    "        next_state = [self.num_shards, self.block_size, self.block_interval, self.malicious_ratio]\n",
    "\n",
    "        return (next_state, reward, done)\n",
    "\n",
    "    def calculate_tps(self, num_shards, block_size, block_interval):\n",
    "        \"\"\"Calculate the transactions per second (TPS) based on shard count and block size\"\"\"\n",
    "        return num_shards * (block_size / block_interval) * self.tps_baseline\n",
    "\n",
    "    def calculate_latency(self, block_size, num_shards):\n",
    "        \"\"\"Estimate the latency based on block size and number of shards\"\"\"\n",
    "        return (block_size * num_shards) / (self.tps_baseline * num_shards)\n",
    "\n",
    "    def calculate_security(self, num_shards, malicious_ratio):\n",
    "        \"\"\"Calculate the security level based on the number of shards and malicious ratio\"\"\"\n",
    "        # If malicious nodes are more than 1/3 in any shard, security is compromised\n",
    "        if malicious_ratio > (1 / 3):\n",
    "            return 0  # Security failure\n",
    "        else:\n",
    "            # Otherwise, the security score is inversely proportional to malicious ratio\n",
    "            return 1 - malicious_ratio\n",
    "\n",
    "    def compute_reward(self, tps, latency, security):\n",
    "        \"\"\"Compute the reward for the current state based on TPS, latency, and security\"\"\"\n",
    "        # Reward is a combination of high TPS, low latency, and high security\n",
    "        return tps * security - latency\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the blockchain environment\"\"\"\n",
    "        print(f\"Shards: {self.num_shards}, Block Size: {self.block_size}MB, Block Interval: {self.block_interval}s\")\n",
    "        print(f\"Malicious Ratio: {self.malicious_ratio}, Total Transactions: {self.total_transactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlockchainEnvironment(num_shards=1, block_size=1, block_interval=1, malicious_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "# total_step = 0\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = random.choice(env.action_space)  # Replace with DQN action later\n",
    "#     next_state, reward, done = env.step(action)\n",
    "#     total_step += 1\n",
    "#     env.render()\n",
    "#     print(f\"Reward: {reward}\\n\")\n",
    "\n",
    "# print(f\"Total steps: {total_step}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQNSB Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSBAgent:\n",
    "    def __init__(self, state_size, action_size, memory_capacity, gamma=0.99, epsilon=1.0, epsilon_decay=0.98, epsilon_min=0.01, alpha=0.1):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayMemory(memory_capacity)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # Decay rate for epsilon\n",
    "        self.epsilon_min = epsilon_min  # Minimum epsilon\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.main_q_network = self.build_q_network()\n",
    "        self.target_q_network = self.build_q_network()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def build_q_network(self):\n",
    "        # Build the Q-network (main and target have the same architecture)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.alpha), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Update the target network weights to match the main network weights\n",
    "        self.target_q_network.set_weights(self.main_q_network.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Random action\n",
    "        state = np.reshape(state, self.state_size)\n",
    "        q_values = self.main_q_network.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])  # Action with the highest Q-value\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_batch_from_buffer(self, batch_size):\n",
    "        exp_batch = self.memory.sample(batch_size=batch_size)\n",
    "        state_batch = np.array([batch[0] for batch in exp_batch]).reshape(batch_size, self.state_size)\n",
    "        action_batch = np.array([batch[1] for batch in exp_batch])\n",
    "        reward_batch = [batch[2] for batch in exp_batch]\n",
    "        next_state_batch = np.array([batch[3] for batch in exp_batch]).reshape(batch_size, self.state_size)\n",
    "        terminal_batch = [batch[4] for batch in exp_batch]\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Sample a minibatch from the memory\n",
    "        if self.memory.__len__() < batch_size:\n",
    "            return\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = self.get_batch_from_buffer(batch_size=batch_size)\n",
    "\n",
    "        # Get current Q value \n",
    "        q_values = self.main_q_network.predict(state_batch, verbose=0)\n",
    "        \n",
    "        # # Get Max Q value of State S' (S' <- S, A)\n",
    "        next_q_values = self.target_q_network.predict(next_state_batch, verbose=0)\n",
    "        max_next_q = np.amax(next_q_values, axis=1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Bellman equation for Q-learning\n",
    "            new_q_values = reward_batch[i] if terminal_batch[i] else reward_batch[i] + self.gamma * max_next_q[i]\n",
    "            q_values[0][action_batch[i]] = new_q_values\n",
    "\n",
    "        #! can not decay the epsilon\n",
    "        self.main_q_network.fit(state_batch, q_values, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        # Load model weights (if any)\n",
    "        self.main_q_network.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        # Save model weights\n",
    "        self.main_q_network.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Hyperparameters and utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Define the number of epochs (episodes) and the maximum steps per episode\n",
    "epochs = 500\n",
    "max_steps = 1000\n",
    "\n",
    "# Example usage\n",
    "state_size = 4  # num_shards, block_size, block_interval, malicious_ratio\n",
    "action_size = 2  # shard change, block interval change\n",
    "\n",
    "agent = DQNSBAgent(state_size, action_size, memory_capacity=20000)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    state = env.reset()\n",
    "    # state = np.array(state)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for step in range(max_steps):\n",
    "        # Agent select an action\n",
    "        action = agent.act(state=state)\n",
    "        # Execute action A^t in the environment and observe the outcome\n",
    "        next_state, reward, done = env.step(action=action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        # Store the experience in replay memory\n",
    "        agent.remember(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        # Perform replay to train the agent\n",
    "        agent.replay(BATCH_SIZE)\n",
    "\n",
    "        # Check if the epoch has ended\n",
    "        if done:\n",
    "            episode_durations.append(step + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "        \n",
    "    # Update the target network periodically\n",
    "    if epoch % 10 == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Save weights\n",
    "agent.save(\"DQNSB_agent.h5\")\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
