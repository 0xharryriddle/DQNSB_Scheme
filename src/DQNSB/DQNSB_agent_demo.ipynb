{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple replay memory class\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background blockchain Environment (BCenv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockchainEnvironment:\n",
    "    def __init__(self):\n",
    "        self.block_size_max = 10\n",
    "        self.block_interval_max = 10\n",
    "        self.shard_max = 5\n",
    "        self.num_nodes = 10\n",
    "        self.u = 0.9\n",
    "        self.security_threshold = 0.8\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns the initial state\"\"\"\n",
    "        R = self.get_transmission_rate()\n",
    "        c = self.get_computing_power()\n",
    "        H = self.get_consensus_history()\n",
    "        p_bar = self.estimate_malicious_prob(H)\n",
    "        self.state = np.array([R, c, H, p_bar])\n",
    "        return self.state\n",
    "\n",
    "    def get_transmission_rate(self):\n",
    "        return np.random.random(self.num_nodes).mean()\n",
    "\n",
    "    def get_computing_power(self):\n",
    "        return np.random.random(self.num_nodes).mean()\n",
    "\n",
    "    def get_consensus_history(self):\n",
    "        return np.random.choice([0, 1], size=self.num_nodes).mean()\n",
    "\n",
    "    def estimate_malicious_prob(self, H):\n",
    "        trust_level = np.mean(H)\n",
    "        return 1 - trust_level\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action and returns the next state, reward, and done flag\"\"\"\n",
    "        B, T_I, K_star = action\n",
    "        R, c, H, p_bar = self.state\n",
    "        \n",
    "        # Simulate next state and reward calculation based on the environment rules\n",
    "        T_latency = T_I + np.random.random()\n",
    "        latency_ok = (T_latency <= self.u * T_I)\n",
    "        security_ok = (K_star < self.security_threshold * np.random.random())\n",
    "        \n",
    "        reward = 0\n",
    "        if latency_ok and security_ok:\n",
    "            reward = 1  # Reward of 1 if both conditions are met\n",
    "        else:\n",
    "            reward = -1  # Negative reward if any condition is violated\n",
    "\n",
    "        # Update state\n",
    "        next_state = self.reset()\n",
    "        done = False  # For simplicity, no terminal state is defined\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def sample_action(self):\n",
    "        \"\"\"Sample a random action for exploration\"\"\"\n",
    "        block_size = np.random.randint(1, self.block_size_max+1)\n",
    "        block_interval = np.random.randint(1, self.block_interval_max+1)\n",
    "        shard_number = np.random.randint(1, self.shard_max+1)\n",
    "        return [block_size, block_interval, shard_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, lr=1e-3):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.model = DQN(state_size, action_size).to(device)\n",
    "        self.target_model = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.model(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_values = self.target_model(next_state_batch).max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = reward_batch + (self.gamma * next_state_values)\n",
    "\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Hyperparameters and utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
